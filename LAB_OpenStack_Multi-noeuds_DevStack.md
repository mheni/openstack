# LAB : Installation OpenStack Multi-n≈ìuds avec DevStack

---

## üìã Objectifs du LAB

√Ä l'issue de ce LAB, vous serez capable de :

‚úÖ Configurer un environnement de virtualisation VMware Workstation pour OpenStack  
‚úÖ D√©ployer une architecture OpenStack multi-n≈ìuds (1 controller + 2 compute + 1 storage)  
‚úÖ Ma√Ætriser la configuration r√©seau multi-interfaces (NAT, Provider, Management)  
‚úÖ Installer et configurer DevStack en mode distribu√©  
‚úÖ V√©rifier et valider les services OpenStack (Nova, Neutron, Glance, Cinder, Horizon)  
‚úÖ Cr√©er et g√©rer des instances virtuelles et volumes de stockage  
‚úÖ Diagnostiquer et r√©soudre les probl√®mes courants d'installation

---

## üìö Table des mati√®res

1. [Architecture du lab](#architecture-du-lab)
2. [PARTIE 1 : Pr√©paration VMware Workstation](#partie-1--pr√©paration-vmware-workstation)
   - [1.1 Cr√©er les r√©seaux virtuels](#11-cr√©er-les-r√©seaux-virtuels)
   - [1.2 Cr√©er les 4 VMs Ubuntu 22.04](#12-cr√©er-les-4-vms-ubuntu-2204)
3. [PARTIE 2 : Configuration r√©seau et syst√®me](#partie-2--configuration-r√©seau-et-syst√®me)
   - [2.1 Hostnames](#21-hostnames)
   - [2.2 Identifier les interfaces](#22-identifier-les-interfaces)
   - [2.3 Plan d'adressage](#23-plan-dadressage)
   - [2.4 Configuration Netplan](#24-configuration-netplan)
   - [2.5 Configuration /etc/hosts](#25-configuration-etchosts)
   - [2.6 Tests de connectivit√©](#26-tests-de-connectivit√©)
4. [PARTIE 3 : Pr√©paration DevStack](#partie-3--pr√©paration-devstack)
   - [3.1 Cr√©er l'utilisateur stack](#31-cr√©er-lutilisateur-stack-sur-les-4-vms)
   - [3.2 Configuration SSH](#32-configuration-ssh-controller--tous-les-n≈ìuds)
5. [PARTIE 4 : Installation DevStack ‚Äì Controller](#partie-4--installation-devstack--controller)
   - [4.1 T√©l√©charger DevStack](#41-t√©l√©charger-devstack)
   - [4.2 Fichier local.conf](#42-fichier-localconf)
   - [4.3 Installation](#43-installation)
   - [4.4 V√©rifications](#44-v√©rifications)
6. [PARTIE 5 : Installation DevStack ‚Äì Compute1](#partie-5--installation-devstack--compute1)
7. [PARTIE 6 : Installation DevStack ‚Äì Compute2](#partie-6--installation-devstack--compute2)
8. [PARTIE 7 : Installation DevStack ‚Äì Block1](#partie-7--installation-devstack--block1)
9. [PARTIE 8 : Enregistrement et v√©rifications](#partie-8--enregistrement-et-v√©rifications)
   - [8.1 Enregistrer les compute nodes](#81-enregistrer-les-compute-nodes)
   - [8.2 V√©rifier les agents r√©seau](#82-v√©rifier-les-agents-r√©seau)
   - [8.3 V√©rifier Cinder](#83-v√©rifier-cinder)
10. [PARTIE 9 : Tests fonctionnels](#partie-9--tests-fonctionnels)
    - [9.1 Dashboard Horizon](#91-dashboard-horizon)
    - [9.2 Test volume](#92-test-volume)
    - [9.3 Test instances](#93-test-instances)
11. [ANNEXE : D√©pannage](#annexe--d√©pannage)

---

## Architecture du lab

```
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ        Internet (NAT - VMnet8)           ‚îÇ
                ‚îÇ          10.10.10.0/24                   ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ DHCP
                ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                ‚îÇ              ‚îÇ                           ‚îÇ
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ Controller ‚îÇ  ‚îÇ Compute1  ‚îÇ  ‚îÇ Compute2  ‚îÇ  ‚îÇ  Block1   ‚îÇ
          ‚îÇ 10.0.0.11  ‚îÇ  ‚îÇ 10.0.0.31 ‚îÇ  ‚îÇ 10.0.0.32 ‚îÇ  ‚îÇ 10.0.0.41 ‚îÇ
          ‚îÇ (ens38)    ‚îÇ  ‚îÇ (ens38)   ‚îÇ  ‚îÇ (ens38)   ‚îÇ  ‚îÇ (ens38)   ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ               ‚îÇ              ‚îÇ              ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ          Management Network (Host-Only - VMnet3)                  ‚îÇ
   ‚îÇ                    10.0.0.0/24                                    ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ          Provider Network (Host-Only - VMnet2)                    ‚îÇ
   ‚îÇ                  203.0.113.0/24                                   ‚îÇ
   ‚îÇ    Controller: 203.0.113.11  |  Compute1: 203.0.113.31           ‚îÇ
   ‚îÇ    Compute2: 203.0.113.32    |  Block1: 203.0.113.41             ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Services par n≈ìud

```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Controller  ‚îÇ MySQL, RabbitMQ, Keystone, Nova API, Glance, Horizon,   ‚îÇ
‚îÇ             ‚îÇ Neutron API, Placement, Cinder API, OVN Northd          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Compute1/2  ‚îÇ nova-compute, OVN Controller, OVS, Metadata Agent       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Block1      ‚îÇ cinder-volume (LVM), cinder-scheduler, cinder-backup    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Caract√©ristiques techniques :**
- **OS** : Ubuntu Server 22.04 LTS
- **DevStack version** : 2026.1
- **Dur√©e estim√©e** : 3‚Äì4 heures
- **Pr√©requis** : VMware Workstation, 32 Go RAM h√¥te minimum, 200 Go disque

---

## PARTIE 1 : Pr√©paration VMware Workstation

### 1.1 Cr√©er les r√©seaux virtuels

**Menu VMware** : `Edit ‚Üí Virtual Network Editor ‚Üí Run as Administrator`

Cr√©er **3 r√©seaux** :

| R√©seau | VMnet | Type | Subnet | Masque | DHCP |
|--------|-------|------|--------|--------|------|
| **Internet** | VMnet8 | NAT | 10.10.10.0 | 255.255.255.0 | Yes |
| **Provider** | VMnet2 | Host-Only | 203.0.113.0 | 255.255.255.0 | Yes |
| **Management** | VMnet3 | Host-Only | 10.0.0.0 | 255.255.255.0 | Yes |

**R√©sultat attendu** : 3 r√©seaux visibles dans Virtual Network Editor.

---

### 1.2 Cr√©er les 4 VMs Ubuntu 22.04

#### Sp√©cifications minimales

| VM | vCPU | RAM | Disque | Cartes r√©seau |
|----|------|-----|--------|---------------|
| **controller** | 4 | 8 Go | 60 Go | 3 (NAT + VMnet2 + VMnet3) |
| **compute1** | 4 | 8 Go | 50 Go | 3 (NAT + VMnet2 + VMnet3) |
| **compute2** | 4 | 8 Go | 50 Go | 3 (NAT + VMnet2 + VMnet3) |
| **block1** | 2 | 4 Go | 50 Go | 3 (NAT + VMnet2 + VMnet3) |

#### Proc√©dure pour chaque VM

1. **File ‚Üí New Virtual Machine ‚Üí Custom**
2. S√©lectionner **Ubuntu Server 22.04 ISO**
3. Configurer les ressources (vCPU, RAM, disque)
4. Dans **VM Settings ‚Üí Network Adapter**, ajouter 3 cartes :
   - **Adapter 1** ‚Üí NAT (VMnet8)
   - **Adapter 2** ‚Üí Custom: VMnet2 (Provider)
   - **Adapter 3** ‚Üí Custom: VMnet3 (Management)

#### Installation Ubuntu

- Utilisateur syst√®me : `stagiaire`
- Installer OpenSSH server : **Yes**
- Pas de packages suppl√©mentaires
- Partitionnement : Disque entier (pas de LVM)

**R√©p√©ter** pour les 4 VMs : controller, compute1, compute2, block1.

---

## PARTIE 2 : Configuration r√©seau et syst√®me

### 2.1 Hostnames

Sur **chaque VM** (en tant que root ou avec sudo) :

```bash
# Sur Controller
sudo hostnamectl set-hostname controller

# Sur Compute1
sudo hostnamectl set-hostname compute1

# Sur Compute2
sudo hostnamectl set-hostname compute2

# Sur Block1
sudo hostnamectl set-hostname block1
```

D√©connecte-toi et reconnecte-toi pour voir le changement dans le prompt.

**R√©sultat attendu** : Le prompt affiche le nouveau hostname.

---

### 2.2 Identifier les interfaces

Sur chaque VM :

```bash
ip a
```

**R√©sultat attendu** :

- `lo` : loopback (127.0.0.1)
- `ens33` : NAT (DHCP automatique, ~10.10.10.x)
- `ens37` : Provider (pas d'IP configur√©e)
- `ens38` : Management (pas d'IP configur√©e)

‚ö†Ô∏è **Important** : Si les noms d'interfaces sont diff√©rents (ex: ens160, ens192), note-les et adapte tous les fichiers Netplan ci-dessous avec tes noms r√©els.

---

### 2.3 Plan d'adressage

| N≈ìud | Interface NAT (ens33) | Interface Provider (ens37) | Interface Management (ens38) |
|------|---------------------|--------------------------|----------------------------|
| controller | DHCP auto | 203.0.113.11/24 | 10.0.0.11/24 |
| compute1 | DHCP auto | 203.0.113.31/24 | 10.0.0.31/24 |
| compute2 | DHCP auto | 203.0.113.32/24 | 10.0.0.32/24 |
| block1 | DHCP auto | 203.0.113.41/24 | 10.0.0.41/24 |

**R√¥le des r√©seaux :**

- **NAT (ens33)** : Acc√®s Internet pour t√©l√©charger les packages
- **Provider (ens37)** : R√©seau externe OpenStack (floating IPs)
- **Management (ens38)** : Communication inter-n≈ìuds OpenStack

---

### 2.4 Configuration Netplan

#### Controller ‚Äì /etc/netplan/01-netcfg.yaml

```bash
sudo nano /etc/netplan/01-netcfg.yaml
```

Remplacer le contenu par :

```yaml
network:
  version: 2
  ethernets:
    ens33:
      dhcp4: true
      nameservers:
        addresses: [8.8.8.8]
    ens36:
      dhcp4: false
      addresses: [203.0.113.11/24]
    ens38:
      dhcp4: false
      addresses: [10.0.0.11/24]

```

Appliquer :

```bash
sudo systemctl enable systemd-networkd
sudo netplan apply
ip addr show
```

**R√©sultat attendu** :

- `ens33` : IP ~10.10.10.x (DHCP)
- `ens37` : IP 203.0.113.11/24
- `ens38` : IP 10.0.0.11/24

---

#### Compute1 ‚Äì /etc/netplan/01-netcfg.yaml

```yaml
network:
  version: 2
  ethernets:
    ens33:
      dhcp4: true
      nameservers:
        addresses: [8.8.8.8]
    ens36:
      dhcp4: false
      addresses: [203.0.113.31/24]
    ens38:
      dhcp4: false
      addresses: [10.0.0.31/24]

```

```bash
sudo netplan apply
ip addr show
```

---

#### Compute2 ‚Äì /etc/netplan/01-netcfg.yaml

```yaml
network:
  version: 2
  ethernets:
    ens33:
      dhcp4: true
      nameservers:
        addresses: [8.8.8.8]
    ens36:
      dhcp4: false
      addresses: [203.0.113.32/24]
    ens38:
      dhcp4: false
      addresses: [10.0.0.32/24]

```

```bash
sudo netplan apply
ip addr show
```

---

#### Block1 ‚Äì /etc/netplan/01-netcfg.yaml

```yaml
network:
  version: 2
  ethernets:
    ens33:
      dhcp4: true
      nameservers:
        addresses: [8.8.8.8]
    ens36:
      dhcp4: false
      addresses: [203.0.113.41/24]
    ens38:
      dhcp4: false
      addresses: [10.0.0.41/24]

```

```bash
sudo netplan apply
ip addr show
```

---

### 2.5 Configuration /etc/hosts

Sur toutes les VMs, √©diter `/etc/hosts` :

```bash
sudo nano /etc/hosts
```

Ajouter (en conservant la ligne `127.0.0.1 localhost`) :

```
127.0.0.1   localhost

# Management Network OpenStack
10.0.0.11   controller
10.0.0.31   compute1
10.0.0.32   compute2
10.0.0.41   block1
```

Sauvegarder et quitter.

---

### 2.6 Tests de connectivit√©

Sur chaque VM, effectuer les tests suivants :

#### Test 1 : Acc√®s Internet

```bash
ping -c 3 openstack.org
```

**R√©sultat attendu** : 3 r√©ponses re√ßues.

#### Test 2 : R√©solution par hostname

```bash
ping -c 3 controller
ping -c 3 compute1
ping -c 3 compute2
ping -c 3 block1
```

**R√©sultat attendu** : Toutes les commandes re√ßoivent des r√©ponses.

#### Test 3 : Ping par IP Management

```bash
ping -c 3 10.0.0.11
ping -c 3 10.0.0.31
ping -c 3 10.0.0.32
ping -c 3 10.0.0.41
```

**R√©sultat attendu** : Toutes les IP Management r√©pondent.

‚ö†Ô∏è **POINT DE CONTR√îLE** : Si un test √©choue, ne pas continuer. V√©rifier Netplan, /etc/hosts, et la configuration VMware (tous les n≈ìuds doivent √™tre sur VMnet2 et VMnet3).

---

## PARTIE 3 : Pr√©paration DevStack

### 3.1 Cr√©er l'utilisateur stack (sur les 4 VMs)

Sur chaque VM (controller, compute1, compute2, block1) :

```bash
sudo apt-get update
sudo apt-get install -y git sudo vim

sudo useradd -s /bin/bash -d /opt/stack -m stack
sudo chmod +x /opt/stack
echo "stack ALL=(ALL) NOPASSWD: ALL" | sudo tee /etc/sudoers.d/stack
sudo passwd stack
```

Mot de passe √† d√©finir : **stack**

Puis devenir utilisateur stack :

```bash
sudo su - stack
pwd
```

**R√©sultat attendu** : `/opt/stack`

---

### 3.2 Configuration SSH (Controller ‚Üí tous les n≈ìuds)

Sur **controller uniquement**, en tant qu'utilisateur **stack** :

#### G√©n√©rer la cl√© SSH

```bash
ssh-keygen -t rsa -N "" -f ~/.ssh/id_rsa
```

**R√©sultat attendu** : Cl√© cr√©√©e dans `/opt/stack/.ssh/id_rsa`.

#### Copier la cl√© vers tous les n≈ìuds

```bash
ssh-copy-id stack@controller
ssh-copy-id stack@compute1
ssh-copy-id stack@compute2
ssh-copy-id stack@block1
```

Mot de passe demand√© : **stack** (pour chaque n≈ìud)

#### V√©rifier l'acc√®s sans mot de passe

```bash
ssh stack@compute1 hostname
ssh stack@compute2 hostname
ssh stack@block1 hostname
```

**R√©sultat attendu** :

- Pas de mot de passe demand√©
- Affichage des hostnames : compute1, compute2, block1

---

## PARTIE 4 : Installation DevStack ‚Äì Controller

### 4.1 T√©l√©charger DevStack

Sur **controller**, connect√© en utilisateur **stack** :

```bash
cd /opt/stack
git clone https://opendev.org/openstack/devstack
cd devstack
```

**R√©sultat attendu** : R√©pertoire `/opt/stack/devstack` cr√©√©.

---

### 4.2 Fichier local.conf

```bash
[[local|localrc]]

HOST_IP=10.0.0.11

FIXED_RANGE=10.11.12.0/24
FLOATING_RANGE=10.0.0.200/27
PUBLIC_NETWORK_GATEWAY=10.0.0.1

ADMIN_PASSWORD=openstack
DATABASE_PASSWORD=openstack
RABBIT_PASSWORD=openstack
SERVICE_PASSWORD=openstack

LOGFILE=/opt/stack/logs/stack.sh.log
LOG_COLOR=False

# Services de base
ENABLED_SERVICES=mysql,rabbit,key

# Nova (sans n-cpu qui sera sur les compute nodes)
ENABLED_SERVICES+=,n-api,n-cond,n-sch,n-novnc,n-api-meta

# Placement
ENABLED_SERVICES+=,placement-api,placement-client

# Glance
ENABLED_SERVICES+=,g-api

# Cinder API + scheduler (obligatoire pour que block1 fonctionne)
ENABLED_SERVICES+=,c-api,c-sch

# Neutron avec OVN
ENABLED_SERVICES+=,q-svc,q-meta
ENABLED_SERVICES+=,ovn-controller,ovn-northd,ovs-vswitchd,ovsdb-server,q-ovn-metadata-agent

# Horizon
ENABLED_SERVICES+=,horizon

Q_USE_PROVIDERNET_FOR_PUBLIC=True
PUBLIC_INTERFACE=ens38

NOVA_VNC_ENABLED=True
VNCSERVER_LISTEN=0.0.0.0
VNCSERVER_PROXYCLIENT_ADDRESS=$HOST_IP

```

V√©rifier le fichier :

```bash
cat local.conf | head -20
```

---

### 4.3 Installation

```bash
./stack.sh
```

**Dur√©e** : 20‚Äì40 minutes selon la connexion Internet

**R√©sultat attendu en fin d'installation** :

```
This is your host IP address: 10.0.0.11
Horizon is now available at http://10.0.0.11/dashboard
Keystone is serving at http://10.0.0.11/identity/
The default users are: admin and demo
The password: openstack

DevStack Version: 2026.1
OS Version: Ubuntu 22.04 jammy
```

---

### 4.4 V√©rifications

#### V√©rifier les services OpenStack

```bash
source /opt/stack/devstack/openrc admin admin
openstack service list
```

**R√©sultat attendu** : Services keystone, nova, glance, neutron, placement, cinderv3 list√©s.

#### V√©rifier les endpoints

```bash
openstack endpoint list
#modifier les adresse si vous les trouvez 127.0.0.1:60999 ou 9292 pour eviter les probl√®me




```

**R√©sultat attendu** : Endpoints public, internal, admin pour chaque service avec IP 10.0.0.11.

#### V√©rifier que Cinder API √©coute

```bash
sudo ss -ltnp | grep 8776
sudo sed -i 's/http-socket = 127.0.0.1:60999/http-socket = 0.0.0.0:9292/' /etc/glance/glance-uwsgi.ini
grep http-socket /etc/glance/glance-uwsgi.ini
sudo systemctl restart devstack@g-api
sudo ss -ltnp | grep 9292
r√©sultat pour ce ci LISTEN  0  100  0.0.0.0:9292  0.0.0.0:*  users:(("uwsgi",...))

```

**R√©sultat attendu** : Port 8776 en LISTEN sur 10.0.0.11.


---

## PARTIE 5 : Installation DevStack ‚Äì Compute1

### 5.1 T√©l√©charger DevStack

Sur **compute1**, en utilisateur **stack** :

```bash
cd /opt/stack
git clone https://opendev.org/openstack/devstack
cd devstack
```

---

### 5.2 Fichier local.conf

```bash
cat > local.conf << 'EOF'
[[local|localrc]]

HOST_IP=10.0.0.31

FIXED_RANGE=10.11.12.0/24
FLOATING_RANGE=10.0.0.200/27

ADMIN_PASSWORD=openstack
DATABASE_PASSWORD=openstack
RABBIT_PASSWORD=openstack
SERVICE_PASSWORD=openstack

LOGFILE=/opt/stack/logs/stack.sh.log

SERVICE_HOST=10.0.0.11
MYSQL_HOST=$SERVICE_HOST
RABBIT_HOST=$SERVICE_HOST
GLANCE_HOSTPORT=$SERVICE_HOST:9292
KEYSTONE_AUTH_HOST=$SERVICE_HOST
KEYSTONE_SERVICE_HOST=$SERVICE_HOST

ENABLED_SERVICES=n-cpu,placement-client,ovn-controller,ovs-vswitchd,ovsdb-server,q-ovn-metadata-agent

NOVA_VNC_ENABLED=True
NOVNCPROXY_URL="http://$SERVICE_HOST:6080/vnc_auto.html"
VNCSERVER_LISTEN=$HOST_IP
VNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTEN
EOF


---

### 5.3 Installation

```bash
./stack.sh
```

**Dur√©e** : 15‚Äì25 minutes

**R√©sultat attendu** :

```
This is your host IP address: 10.0.0.31
DevStack Version: 2026.1
OS Version: Ubuntu 22.04 jammy
```

---

## PARTIE 6 : Installation DevStack ‚Äì Compute2

### 6.1 T√©l√©charger DevStack

Sur **compute2**, en utilisateur **stack** :

```bash
cd /opt/stack
git clone https://opendev.org/openstack/devstack
cd devstack
```

---

### 6.2 Fichier local.conf

```bash
cat > local.conf << 'EOF'
[[local|localrc]]

# IP Management de Compute2
HOST_IP=10.0.0.32

# R√©seaux internes (identiques au controller)
FIXED_RANGE=10.11.12.0/24
FLOATING_RANGE=10.0.0.200/27

# Mots de passe (identiques au controller)
ADMIN_PASSWORD=openstack
DATABASE_PASSWORD=openstack
RABBIT_PASSWORD=openstack
SERVICE_PASSWORD=openstack

LOGFILE=/opt/stack/logs/stack.sh.log

# Multi-n≈ìud : pointer vers le controller
SERVICE_HOST=10.0.0.11
MYSQL_HOST=$SERVICE_HOST
RABBIT_HOST=$SERVICE_HOST
GLANCE_HOSTPORT=$SERVICE_HOST:9292
KEYSTONE_AUTH_HOST=$SERVICE_HOST
KEYSTONE_SERVICE_HOST=$SERVICE_HOST

# Services sur compute : Nova compute + OVN
ENABLED_SERVICES=n-cpu,placement-client,ovn-controller,ovs-vswitchd,ovsdb-server,q-ovn-metadata-agent

# Configuration VNC
NOVA_VNC_ENABLED=True
NOVNCPROXY_URL="http://$SERVICE_HOST:6080/vnc_auto.html"
VNCSERVER_LISTEN=$HOST_IP
VNCSERVER_PROXYCLIENT_ADDRESS=$VNCSERVER_LISTEN
EOF

```

---

### 6.3 Installation

```bash
./stack.sh
```

**Dur√©e** : 15‚Äì25 minutes



---

## PARTIE 7 : Installation DevStack ‚Äì Block1

### 7.1 T√©l√©charger DevStack

Sur **block1**, en utilisateur **stack** :

```bash
cd /opt/stack
git clone https://opendev.org/openstack/devstack
cd devstack
```

---

### 7.2 Fichier local.conf

```bash
cat > local.conf << 'EOF'
[[local|localrc]]

# IP Management de block1
HOST_IP=10.0.0.41

# Mots de passe (identiques au controller)
ADMIN_PASSWORD=openstack
DATABASE_PASSWORD=openstack
RABBIT_PASSWORD=openstack
SERVICE_PASSWORD=openstack

LOGFILE=/opt/stack/logs/stack.sh.log

# Multi-n≈ìud : pointer vers le controller
SERVICE_HOST=10.0.0.11
MYSQL_HOST=$SERVICE_HOST
RABBIT_HOST=$SERVICE_HOST
GLANCE_HOSTPORT=$SERVICE_HOST:9292
KEYSTONE_AUTH_HOST=$SERVICE_HOST
KEYSTONE_SERVICE_HOST=$SERVICE_HOST

# Services Cinder : UNIQUEMENT volume, scheduler, backup (PAS c-api)
ENABLED_SERVICES=c-vol,c-sch,c-bak

# Backend LVM pour Cinder
VOLUME_BACKING_FILE_SIZE=50G
VOLUME_BACKING_FILE=/opt/stack/data/stack-volumes-backing-file
VOLUME_GROUP=stack-volumes-lvm
EOF

```

**Note importante** : `c-api` n'est PAS dans ENABLED_SERVICES car il reste sur le controller.

---

### 7.3 Installation

```bash
./stack.sh
```

**Dur√©e** : 10‚Äì20 minutes

**R√©sultat attendu** :

```
This is your host IP address: 10.0.0.41
DevStack Version: 2026.1
OS Version: Ubuntu 22.04 jammy
```

---

## PARTIE 8 : Enregistrement et v√©rifications

### 8.1 Enregistrer les compute nodes

Sur **controller** :

```bash
source /opt/stack/devstack/openrc admin admin
cd /opt/stack/devstack
./tools/discover_hosts.sh
```

**R√©sultat attendu** :

```
Discovering compute hosts...
Found 2 unmapped computes: compute1, compute2
```

V√©rifier :

```bash
openstack compute service list
```

**R√©sultat attendu** :

```
+----+----------------+------------+----------+---------+-------+
| ID | Binary         | Host       | Zone     | Status  | State |
+----+----------------+------------+----------+---------+-------+
|  1 | nova-scheduler | controller | internal | enabled | up    |
|  2 | nova-conductor | controller | internal | enabled | up    |
|  3 | nova-compute   | compute1   | nova     | enabled | up    |
|  4 | nova-compute   | compute2   | nova     | enabled | up    |
+----+----------------+------------+----------+---------+-------+
```

---

### 8.2 V√©rifier les agents r√©seau

```bash
openstack network agent list
```

**R√©sultat attendu** : Agents OVN Controller et Metadata pour controller, compute1, compute2.

---

### 8.3 V√©rifier Cinder

```bash
openstack volume service list
```

**R√©sultat attendu** :

```
+------------------+------------------------+------+---------+-------+
| Binary           | Host                   | Zone | Status  | State |
+------------------+------------------------+------+---------+-------+
| cinder-scheduler | controller             | nova | enabled | up    |
| cinder-volume    | block1@lvm             | nova | enabled | up    |
| cinder-backup    | block1                 | nova | enabled | up    |
+------------------+------------------------+------+---------+-------+
```

---

## PARTIE 9 : Tests fonctionnels

### 9.1 Dashboard Horizon

Ouvrir un navigateur web et acc√©der √† :

```
http://10.0.0.11/dashboard
```

**Connexion** :

- Domain: `default`
- User Name: `admin`
- Password: `openstack`

**R√©sultat attendu** : Dashboard OpenStack s'affiche. Dans Admin > Compute > Hypervisors, tu vois 2 hyperviseurs (compute1, compute2).

---

### 9.2 Test volume

Sur **controller** :

```bash
source /opt/stack/devstack/openrc admin admin
openstack volume create --size 2 test-volume
openstack volume list
```

**R√©sultat attendu** :

```
+--------------------------------------+-------------+-----------+------+-------------+
| ID                                   | Name        | Status    | Size | Attached to |
+--------------------------------------+-------------+-----------+------+-------------+
| xxxxx...                             | test-volume | available |    2 |             |
+--------------------------------------+-------------+-----------+------+-------------+
```

---

### 9.3 Test instances

```bash
# Cr√©er le r√©seau priv√©
openstack network create private-net
openstack subnet create --network private-net --subnet-range 192.168.100.0/24 private-subnet

# Cr√©er et configurer le routeur
openstack router create router1
openstack router add subnet router1 private-subnet
openstack router set --external-gateway public router1

# Cr√©er une paire de cl√©s SSH
openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey

# Cr√©er une instance sur compute1
openstack server create   --flavor m1.tiny   --image cirros-0.6.2-x86_64-disk   --network private-net   --key-name mykey   --availability-zone nova:compute1   vm-compute1

# Cr√©er une instance sur compute2
openstack server create   --flavor m1.tiny   --image cirros-0.6.2-x86_64-disk   --network private-net   --key-name mykey   --availability-zone nova:compute2   vm-compute2

# V√©rifier les instances
openstack server list
```

**R√©sultat attendu** :

```
+--------------------------------------+-------------+--------+------------------------+
| ID                                   | Name        | Status | Networks               |
+--------------------------------------+-------------+--------+------------------------+
| xxxxx...                             | vm-compute1 | ACTIVE | private-net=192.168... |
| xxxxx...                             | vm-compute2 | ACTIVE | private-net=192.168... |
+--------------------------------------+-------------+--------+------------------------+
```

Les deux instances sont en statut ACTIVE, une sur chaque n≈ìud compute.

---

## ANNEXE : D√©pannage

### Probl√®me : Ping entre VMs ne fonctionne pas

**Sympt√¥me** : `ping controller` ou `ping 10.0.0.11` √©choue depuis compute/block

**Solutions** :

- V√©rifier la configuration IP : `ip addr show`, `ip route`
- V√©rifier `/etc/hosts` sur toutes les VMs
- V√©rifier que toutes les VMs sont sur les bons r√©seaux VMware :
  - Adapter 2 ‚Üí VMnet2 (Provider)
  - Adapter 3 ‚Üí VMnet3 (Management)
- Refaire `sudo netplan apply` sur les n≈ìuds concern√©s
- V√©rifier le firewall : `sudo ufw status` (doit √™tre inactif ou bien configur√©)

---

### Probl√®me : DevStack √©choue avec "No route to host"

**Sympt√¥me** : Erreur `No route to host` ou `Connection refused` pendant `./stack.sh`

**Solutions** :

- Depuis le n≈ìud qui √©choue, tester : `ping 10.0.0.11`
- V√©rifier que le service cible √©coute sur controller : `sudo ss -ltnp | grep 8776` (pour Cinder)
- V√©rifier `/etc/hosts` et r√©solution DNS
- Relancer DevStack sur controller si n√©cessaire

---

### Probl√®me : Port 8776 (Cinder API) refuse les connexions

**Sympt√¥me** : Block1 ne peut pas contacter Cinder API sur controller

**Solutions** :

- Sur controller, v√©rifier que `c-api` est dans ENABLED_SERVICES du local.conf
- V√©rifier que le port √©coute : `sudo ss -ltnp | grep 8776`
- V√©rifier les logs : `tail -50 /opt/stack/logs/c-api.log`
- Si n√©cessaire, relancer DevStack sur controller : `cd /opt/stack/devstack && ./stack.sh`

---

### Probl√®me : Compute node appara√Æt en "down"

**Sympt√¥me** : `openstack compute service list` montre compute1 ou compute2 en √©tat "down"

**Solutions** :

- Sur controller : `cd /opt/stack/devstack && ./tools/discover_hosts.sh`
- Attendre 30 secondes et v√©rifier √† nouveau : `openstack compute service list`
- Sur le compute concern√©, v√©rifier les services : `systemctl status devstack@n-cpu`
- Consulter les logs : `tail -50 /opt/stack/logs/n-cpu.log`

---

### Probl√®me : Services OVN manquants

**Sympt√¥me** : `openstack network agent list` ne montre pas les agents OVN

**Solutions** :

- V√©rifier que `q-agt`, `q-l3`, `q-dhcp` ne sont PAS dans ENABLED_SERVICES (incompatibles avec OVN)
- Sur chaque n≈ìud, v√©rifier : `systemctl status devstack@ovn-controller`
- Relancer DevStack si n√©cessaire apr√®s correction du local.conf

---

## FIN DU LAB

**F√©licitations !** Vous disposez maintenant d'un cloud OpenStack multi-n≈ìuds fonctionnel avec :

‚úÖ 1 Controller (API, base de donn√©es, Cinder API)  
‚úÖ 2 Compute nodes (nova-compute + OVN)  
‚úÖ 1 Block storage (Cinder volume/backup LVM)

### Acc√®s au cloud

**Dashboard Horizon** : http://10.0.0.11/dashboard  
**Identifiants** : admin / openstack

### Services disponibles

- Compute (Nova) avec 2 hyperviseurs
- R√©seau (Neutron/OVN)
- Images (Glance)
- Stockage bloc (Cinder)
- Dashboard web (Horizon)

---

**Auteur** : LAB DevStack Multi-n≈ìuds  
**Version** : 2026.1  
**Date** : F√©vrier 2026  
**Licence** : Usage p√©dagogique et formation

---

*Pr√©par√© √† l'aide de Claude Sonnet 4.5*
