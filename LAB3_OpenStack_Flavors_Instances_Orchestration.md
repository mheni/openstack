# LAB 3 : Flavors, Lancement d‚ÄôInstances et Orchestration de D√©ploiements

---

## üéØ Objectifs du LAB

√Ä l‚Äôissue de ce LAB, vous serez capable de :

- Concevoir et g√©rer des **flavors** adapt√©es √† diff√©rents cas d‚Äôusage.
- Lancer des **instances** sur des n≈ìuds de calcul sp√©cifiques (Compute1 / Compute2).
- Organiser un **d√©ploiement orchestr√©** de plusieurs VMs (topologie applicative simple).
- Automatiser le lancement d‚Äôun ensemble d‚Äôinstances via un **script** (pr√©‚Äëorchestration).

Ce lab s‚Äôappuie sur :

- L‚Äôarchitecture DevStack multi‚Äën≈ìuds (Controller + 2 Compute + Block1).
- Les projets et utilisateurs du LAB 1 (`Formation1`, `Formation2`, `AdminLab`).
- Le r√©seau et le stockage mis en place dans le LAB 2 (`f1-net`, `f2-net`, volumes Cinder, floating IPs).

---

## üìö Pr√©requis

Avant de commencer :

1. Les LABs 1 et 2 sont r√©alis√©s (projets, quotas, r√©seaux, security groups, volumes).
2. Au moins une image est disponible pour le boot :
   - `cirros-0.6.2-x86_64-disk` (test rapide) ou
   - une image Ubuntu cloud (pour sc√©narios plus riches).
3. Depuis le **controller** :

```bash
source /opt/stack/devstack/openrc admin admin
openstack hypervisor list
openstack flavor list
openstack image list
```

R√©sultat attendu :

- Deux hyperviseurs (compute1, compute2).
- Quelques flavors par d√©faut (m1.tiny, etc.).
- Au moins une image active.

---

## üß© Sc√©nario p√©dagogique

Vous allez :

1. Cr√©er un **catalogue de flavors** coh√©rent (tiny, small, medium, large).
2. Lancer des instances de types diff√©rents et observer leur placement sur les hyperviseurs.
3. Mettre en place une **mini‚Äëarchitecture 3‚Äëtiers** pour `Formation1` (web + app + db).
4. √âcrire un **script shell** qui d√©ploie automatiquement cette topologie.

Ce LAB vous rapproche de la logique d‚Äô**orchestration** (sans aller jusqu‚Äô√† Heat) et pr√©pare le terrain pour des outils comme Ansible, Terraform ou les templates Heat.

---

## PARTIE 1 : Conception et cr√©ation de flavors

### 1.1 Analyse des hyperviseurs

En tant qu‚Äô`admin` :

```bash
source /opt/stack/devstack/openrc admin admin

# Voir les ressources des hyperviseurs
openstack hypervisor list
openstack hypervisor show compute1
openstack hypervisor show compute2
```

Notez pour chaque hyperviseur : nombre de vCPUs, RAM totale, disque.

### 1.2 D√©finition d‚Äôun catalogue de flavors

Proposition de flavors :

| Flavor       | vCPUs | RAM (Mo) | Disque (Go) | Usage pr√©vu                  |
|--------------|-------|----------|-------------|------------------------------|
| `demo.tiny`  | 1     | 512      | 5           | Tests, Cirros, ping/SSH      |
| `demo.small` | 1     | 2048     | 10          | Petites apps / bastion       |
| `demo.medium`| 2     | 4096     | 20          | App serveur web / API        |
| `demo.large` | 4     | 8192     | 40          | DB / appli lourde (formation)|

Cr√©ez ces flavors :

```bash
# Id√©es de IDs : 101, 102, 103, 104 (optionnel)
openstack flavor create demo.tiny   --id 101 --vcpus 1 --ram 512   --disk 5
openstack flavor create demo.small  --id 102 --vcpus 1 --ram 2048  --disk 10
openstack flavor create demo.medium --id 103 --vcpus 2 --ram 4096  --disk 20
openstack flavor create demo.large  --id 104 --vcpus 4 --ram 8192  --disk 40

# V√©rifier
openstack flavor list
```

### 1.3 (Option) Limitation de flavors par projet

OpenStack permet de rendre certains flavors priv√©s et de les associer √† un projet donn√©.

Exemple : r√©server `demo.large` au projet `AdminLab` (facultatif) :

```bash
# Rendre demo.large priv√©
openstack flavor set demo.large --property "class=adminlab" --private

# Associer le flavor √† un projet
openstack flavor set demo.large --project AdminLab
```

> Exercice : tester depuis `user1` si `demo.large` est visible ou non selon la configuration.

---

## PARTIE 2 : Lancement d‚Äôinstances cibl√©es par hyperviseur

### 2.1 Contexte Formation1 (user1)

Sur le controller :

```bash
cat > user1-openrc.sh << 'EOF'
export OS_AUTH_URL=http://10.0.0.11/identity
export OS_PROJECT_NAME=Formation1
export OS_USERNAME=user1
export OS_PASSWORD=user1pass
export OS_USER_DOMAIN_NAME=Default
export OS_PROJECT_DOMAIN_NAME=Default
export OS_IDENTITY_API_VERSION=3
EOF

source user1-openrc.sh

# V√©rifier
openstack token issue
openstack flavor list
openstack network list
```

Assurez-vous que `f1-net` est pr√©sent et que les flavors `demo.*` sont visibles.

### 2.2 Lancer une instance sur compute1

Nous allons utiliser l‚Äôoption `--availability-zone nova:compute1`.

```bash
# Variables d'aide (adapter l'image si besoin)
IMG="cirros-0.6.2-x86_64-disk"
NET="f1-net"
KEY="mykey"   # √† cr√©er si n√©cessaire

# Instance sur compute1
openstack server create   --flavor demo.small   --image "$IMG"   --network "$NET"   --key-name "$KEY"   --availability-zone nova:compute1   f1-web-compute1

# V√©rifier
openstack server list
openstack server show f1-web-compute1 -f yaml | grep hypervisor
```

**R√©sultat attendu** : l‚Äôinstance `f1-web-compute1` est ACTIVE et plac√©e sur `compute1`.

### 2.3 Lancer une instance sur compute2

```bash
openstack server create   --flavor demo.small   --image "$IMG"   --network "$NET"   --key-name "$KEY"   --availability-zone nova:compute2   f1-web-compute2

openstack server show f1-web-compute2 -f yaml | grep hypervisor
```

**R√©sultat attendu** : `f1-web-compute2` est sur `compute2`.

> Question : que se passe-t-il si compute2 est down (service `n-cpu` arr√™t√©) et que vous relancez la commande ?

---

## PARTIE 3 : Mini‚Äëarchitecture 3‚Äëtiers pour Formation1

Objectif : d√©ployer une petite topologie :

- 1 VM `f1-front` (web) ‚Äì flavor `demo.small`.
- 1 VM `f1-app` (application) ‚Äì flavor `demo.medium`.
- 1 VM `f1-db` (base de donn√©es) ‚Äì flavor `demo.large`.

Toutes sur `f1-net`, mais r√©parties sur les deux hyperviseurs.

### 3.1 Pr√©paration ‚Äì Security groups

On r√©utilise le security group `ssh-only` du LAB 2 ou on en cr√©e un nouveau `web-ssh-db` (exercice possible). Pour simplifier, utilisons `ssh-only` :

```bash
source user1-openrc.sh
openstack security group list
```

Assurez-vous que `ssh-only` existe et autorise au minimum SSH et ICMP.

### 3.2 D√©ploiement des 3 VMs

```bash
# f1-front sur compute1
openstack server create   --flavor demo.small   --image "$IMG"   --network "$NET"   --key-name "$KEY"   --security-group ssh-only   --availability-zone nova:compute1   f1-front

# f1-app sur compute2
openstack server create   --flavor demo.medium   --image "$IMG"   --network "$NET"   --key-name "$KEY"   --security-group ssh-only   --availability-zone nova:compute2   f1-app

# f1-db sur compute2 (par exemple)
openstack server create   --flavor demo.large   --image "$IMG"   --network "$NET"   --key-name "$KEY"   --security-group ssh-only   --availability-zone nova:compute2   f1-db

# V√©rifier l'√©tat global
openstack server list
```

### 3.3 Attribution de floating IPs

Attribuez une floating IP au front (et √©ventuellement √† l‚Äôapp) :

```bash
# Cr√©er une floating IP
FIP_FRONT=$(openstack floating ip create -f value -c floating_ip_address public)

# Associer √† f1-front
openstack server add floating ip f1-front "$FIP_FRONT"

# V√©rifier
openstack server show f1-front -f yaml | egrep 'addresses|floating'
```

Depuis l‚Äôh√¥te (ou une machine ayant acc√®s au r√©seau provider) :

```bash
ping "$FIP_FRONT"
ssh cirros@"$FIP_FRONT"   # ou ubuntu@ selon l'image
```

> Exercice avanc√© :
> - Installer un serveur web sur `f1-front` et une base sur `f1-db` (si image Ubuntu cloud).
> - Tester la communication interne (ping entre VMs, acc√®s DB depuis `f1-app`).

---

## PARTIE 4 : Script d‚Äô‚Äúorchestration‚Äù simple

Objectif : √©crire un script shell qui d√©ploie toute la stack `Formation1` automatiquement :

- Cr√©e le r√©seau `f1-net` et `f1-subnet` (si non existants).
- Cr√©e les flavors `demo.*` (si absents).
- Lance `f1-front`, `f1-app`, `f1-db` avec la topologie d√©finie.

### 4.1 Exemple de script `deploy_f1_stack.sh`

Sur le **controller** :

```bash
cat > deploy_f1_stack.sh << 'EOF'
#!/usr/bin/env bash
set -e

# Charger l'environnement user1
source ~/user1-openrc.sh

IMG="cirros-0.6.2-x86_64-disk"
NET_NAME="f1-net"
SUBNET_NAME="f1-subnet"
SUBNET_CIDR="192.168.201.0/24"
KEY="mykey"

# 1. R√©seau (idempotent)
if ! openstack network show "$NET_NAME" &>/dev/null; then
  echo "[INFO] Cr√©ation du r√©seau $NET_NAME"
  openstack network create --project Formation1 "$NET_NAME"
  openstack subnet create     --project Formation1     --network "$NET_NAME"     --subnet-range "$SUBNET_CIDR"     --dns-nameserver 8.8.8.8     "$SUBNET_NAME"
fi

# 2. Flavors (idempotent)
create_flavor_if_missing() {
  local name=$1 vcpus=$2 ram=$3 disk=$4 id=$5
  if ! openstack flavor show "$name" &>/dev/null; then
    echo "[INFO] Cr√©ation du flavor $name"
    openstack flavor create "$name" --id "$id" --vcpus "$vcpus" --ram "$ram" --disk "$disk"
  fi
}

create_flavor_if_missing demo.tiny   1 512   5  101
create_flavor_if_missing demo.small  1 2048  10 102
create_flavor_if_missing demo.medium 2 4096  20 103
create_flavor_if_missing demo.large  4 8192  40 104

# 3. Security group ssh-only (idempotent)
if ! openstack security group show ssh-only &>/dev/null; then
  echo "[INFO] Cr√©ation du security group ssh-only"
  openstack security group create ssh-only --description "SSH + ICMP"
  openstack security group rule create --proto tcp --dst-port 22 --ingress ssh-only
  openstack security group rule create --proto icmp --ingress ssh-only
fi

# 4. Lancement des VMs
NET_ID=$(openstack network show "$NET_NAME" -f value -c id)

create_server_if_missing() {
  local name=$1 flavor=$2 az=$3
  if ! openstack server show "$name" &>/dev/null; then
    echo "[INFO] Cr√©ation de la VM $name"
    openstack server create       --flavor "$flavor"       --image "$IMG"       --network "$NET_ID"       --key-name "$KEY"       --security-group ssh-only       --availability-zone "$az"       "$name"
  else
    echo "[INFO] VM $name d√©j√† existante, skip"
  fi
}

create_server_if_missing f1-front demo.small  "nova:compute1"
create_server_if_missing f1-app   demo.medium "nova:compute2"
create_server_if_missing f1-db    demo.large  "nova:compute2"

openstack server list
EOF

chmod +x deploy_f1_stack.sh
```

Ex√©cuter le script :

```bash
./deploy_f1_stack.sh
```

**R√©sultat attendu** :

- Si rien n‚Äôexiste encore, le script cr√©e r√©seau + flavors + security group + VMs.
- Si vous relancez le script, il d√©tecte les √©l√©ments existants et ne les duplique pas (idempotence simple).

### 4.2 Am√©liorations possibles

- Ajouter la cr√©ation d‚Äôun routeur `f1-router` et la configuration de la gateway externe.
- Ajouter l‚Äôallocation et l‚Äôassociation automatique de floating IPs √† `f1-front`.
- Ajouter la cr√©ation/attachement d‚Äôun volume Cinder √† `f1-db`.

---

## PARTIE 5 : Vue d‚Äôensemble admin et contr√¥le de capacit√©

En tant qu‚Äô`admin` :

```bash
source /opt/stack/devstack/openrc admin admin

# Vue globale des instances
openstack server list --all-projects

# Vue des hyperviseurs et ressources consomm√©es
openstack hypervisor stats show
openstack hypervisor list
openstack hypervisor show compute1
openstack hypervisor show compute2

# Vue des flavors
openstack flavor list
```

Analyse :

- V√©rifiez la r√©partition des VMs entre compute1 et compute2.
- Comparez les ressources allou√©es aux capacit√©s restantes.
- Discutez des limites possibles (quotas projet, capacit√©s hyperviseur, overcommit, etc.).

---

## üîé Questions de validation

1. √Ä quoi sert un **flavor** dans OpenStack et quels sont ses param√®tres principaux ?
2. Comment forcer le placement d‚Äôune instance sur un hyperviseur sp√©cifique ?
3. Quels sont les risques d‚Äôun catalogue de flavors mal dimensionn√© ?
4. Qu‚Äôapporte un script d‚Äô‚Äúorchestration‚Äù par rapport √† des commandes manuelles ?
5. Comment v√©rifier l‚Äôimpact de vos d√©ploiements sur la **capacit√©** des hyperviseurs ?

---

## ‚úÖ Bilan du LAB 3

Dans ce troisi√®me LAB, vous avez :

- Construit un catalogue de flavors adapt√© √† des sc√©narios de formation.
- Plac√© des instances sur des hyperviseurs pr√©cis via les availability zones.
- D√©ploy√© une petite architecture multi‚ÄëVM pour un projet donn√©.
- Automatis√© ce d√©ploiement avec un script shell r√©utilisable.
- Appris √† analyser la capacit√© et la r√©partition de charge entre vos compute nodes.

Ce LAB pr√©pare la transition vers de vraies solutions d‚Äôorchestration (Heat, Ansible, Terraform) qui industrialisent le d√©ploiement d‚Äôenvironnements complexes.

---

**Auteur** : DR; ING MAHER HENI LAB DevStack Multi-n≈ìuds ‚Äì Flavors & Orchestration  
**Version** : 1.0  
**Date** : F√©vrier 2026  
**Licence** : Usage p√©dagogique et formation
